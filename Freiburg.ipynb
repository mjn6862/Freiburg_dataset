{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Freiburg",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjn6862/Freiburg_dataset/blob/master/Freiburg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTMpzey5z5c",
        "colab_type": "text"
      },
      "source": [
        "**Comma AI Speed Challenge**\n",
        "\n",
        "  This notebook will contain (hopefully) all of the functions you need to import the data into your model.\n",
        "\n",
        "  ***Be sure to train with GPU acceleration enabled***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kq-rlJxzHbr",
        "colab_type": "text"
      },
      "source": [
        "**Import Statements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sguc70jY5xt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import tensorflow_graphics.geometry.transformation as tfg_transformation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbmO0l8_UAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNzSFHCVDKGp",
        "colab_type": "text"
      },
      "source": [
        "**Custom Data Generator**\n",
        "\n",
        "This works (I think) for giving two sequential images to a Keras Functional model as well as the velocity associated with the second image.\n",
        "\n",
        "At this point, don't worry about how this works. If you need something changed or fixed, just ask. This is the boring part anyways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-q1fd9jHu7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#New from Kyle:\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, custom_indices, batch_size=32,\n",
        "                  shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.direct = \"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/\"\n",
        "        self.indexes = custom_indices\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        index = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        # Find list of IDs\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(index)#batch)\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs)-5)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "    def __data_generation(self, names_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        x1_full = np.empty((0, 480, 640, 4))\n",
        "        x2_full = np.empty((0, 480, 640, 4))\n",
        "        y_full = np.empty((0, 4, 4))\n",
        "        for name in names_temp:\n",
        "          x1 = np.load(self.direct+\"/images/rgbd_%d.npy\"%int(self.list_IDs[name][0]))\n",
        "          x1 = np.reshape(x1,(1,480, 640, 4))\n",
        "          x2 = np.load(self.direct+\"images/rgbd_%d.npy\"%int(self.list_IDs[name+5][0]))\n",
        "          x2 = np.reshape(x2,(1,480, 640, 4))\n",
        "          x1 = x1/255.\n",
        "          x1 = x1 - np.mean(x1, axis=0)\n",
        "          x2 = x2/255.\n",
        "          x2 = x2 - np.mean(x2, axis=0)\n",
        "          y1 = np.load(self.direct+\"pose/htm_%d.npy\"%int(self.list_IDs[name][0]))\n",
        "          y2 = np.load(self.direct+\"pose/htm_%d.npy\"%int(self.list_IDs[name+5][0]))\n",
        "          y = np.linalg.inv(y1).dot(y2)\n",
        "          x1_full = np.concatenate((x1_full, x1))\n",
        "          x2_full = np.concatenate((x2_full, x2))\n",
        "          y_full = np.concatenate((y_full, np.reshape(y, (1, 4, 4)) ))\n",
        "        y_quat = tfg_transformation.quaternion.from_rotation_matrix(y_full[:,0:3, 0:3])\n",
        "        total = tf.keras.layers.concatenate((y_quat, y_full[:,0:3,3]), dtype='float64')\n",
        "        return [x1_full[:,:,:,0:3], x1_full[:,:,:,3], x2_full[:,:,:,0:3], x2_full[:,:,:,3]], total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgc5HQbfFvN1",
        "colab_type": "text"
      },
      "source": [
        "**Define the test-train split and create the Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PVEGfHkIVY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#New from Kyle\n",
        "with open(\"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/indices.csv\", newline='') as f:\n",
        "  reader = csv.reader(f)\n",
        "  list_IDs = list(reader)\n",
        "indexes = np.arange(len(list_IDs)-5)\n",
        "np.random.shuffle(indexes)\n",
        "# Define train/test split\n",
        "train_portion = 0.8\n",
        "train_indices = indexes[0:int(np.floor(len(indexes)*train_portion))]\n",
        "train_IDs = [list_IDs[i] for i in train_indices]\n",
        "valid_indices = indexes[int(np.floor(len(indexes)*train_portion)):]\n",
        "valid_IDs = [list_IDs[i] for i in valid_indices]\n",
        "training_generator = DataGenerator(list_IDs = train_IDs, custom_indices = np.arange(len(train_IDs)), batch_size=16, shuffle=True )\n",
        "validation_generator = DataGenerator(list_IDs = valid_IDs, custom_indices = np.arange(len(valid_IDs)), batch_size=16, shuffle=True )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJy-HdjtEH-o",
        "colab_type": "text"
      },
      "source": [
        "**Define custom loss function**\n",
        "\n",
        "This is not well tested, neither is it optimized. You might not even want to use this function.\n",
        "\n",
        "Keras backend functions are a powerful tool for writing custom loss functions. To define a loss function it just has to accept *y_true* and *y_pred* as arguments and return a float.\n",
        "\n",
        "To use your new loss function, change the argument in *model.compile()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqNfdCNENGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_sq_err(y_true, y_pred):\n",
        "  #y_pred = 15. + y_pred*15.\n",
        "  return tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred)) #+ metric_var(y_true, y_pred)#1./(0.001+tf.keras.backend.var(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI34rmOxb9QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_psuedo_huber(y_true, y_pred):\n",
        "  scale = 10\n",
        "  huber = scale*scale*(tf.keras.backend.sqrt(1+tf.keras.backend.square((y_true-y_pred)/scale)) - 1)\n",
        "  return tf.keras.backend.mean(huber)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hGHhhRpdNO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_var(y_true, y_pred):\n",
        "  #return 1./(0.001+tf.keras.backend.var(y_pred))\n",
        "  epsilon = 0.001\n",
        "  return tf.keras.backend.maximum((epsilon + tf.keras.backend.var(y_true))/(epsilon+tf.keras.backend.var(y_pred)), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6drymOadOAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_psuedo_huber_var(y_true, y_pred):\n",
        "  scale_factor = 10.\n",
        "  return scale_factor1*mean_psuedo_huber(y_true, y_pred) + metric_var(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6qw3x9lB7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_side():\n",
        "  side_input = tf.keras.layers.Input(shape=(480, 640, 4), name=\"side_input\")\n",
        "  pre_split_conv_1 = tf.keras.layers.Conv2D(16, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l1_l2())(side_input)\n",
        "  pre_split_conv_1 = tf.keras.layers.MaxPool2D((2,2))(pre_split_conv_1)\n",
        "  side_output = tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l1_l2())(pre_split_conv_1)\n",
        "  return tf.keras.models.Model(inputs = side_input, outputs = side_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0xuO74GB9N",
        "colab_type": "text"
      },
      "source": [
        "**Define the input layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BszGQ-CEGEDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_A = tf.keras.layers.Input(shape=(480, 640, 3), name=\"first_image\")\n",
        "depth_A = tf.keras.layers.Input(shape=(480, 640, 1), name=\"first_depth\")\n",
        "input_B = tf.keras.layers.Input(shape=(480, 640, 3), name=\"second_image\")\n",
        "depth_B = tf.keras.layers.Input(shape=(480, 640, 1), name=\"second_depth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfgLVnl4GHC1",
        "colab_type": "text"
      },
      "source": [
        "**Define the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2emlkaGmHuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The input to the pose estimation network is the target view concatenated with all the source views \n",
        "(along the color channels), and the outputs are the relative poses between the target view and each \n",
        "of the source views.\n",
        "\"\"\"\n",
        "\n",
        "concat = tf.keras.layers.concatenate([input_A, depth_A, input_B, depth_B])\n",
        "\"\"\"\n",
        "The network consists of 7 stride-2 convolutions \n",
        "\"\"\"\n",
        "conv_1 = tf.keras.layers.Conv2D(16, (7,7), strides=(2,2), activation='relu')(concat)\n",
        "conv_2 = tf.keras.layers.Conv2D(32, (5,5), strides=(2,2), activation='relu')(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv2D(64, (3,3), strides=(2,2), activation='relu')(conv_2)\n",
        "conv_4 = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), activation='relu')(conv_3)\n",
        "conv_5 = tf.keras.layers.Conv2D(256, (3,3), strides=(2,2), activation='relu')(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv2D(512, (3,3), strides=(2,2), activation='relu')(conv_5)\n",
        "conv_7 = tf.keras.layers.Conv2D(1024, (3,3), strides=(2,2), activation='relu')(conv_6)\n",
        "\"\"\"\n",
        "followed by a 1×1 convolution with 6∗(N−1) output channels (corresponding to 3 Euler angles and \n",
        "3-D translation for each source view). \n",
        "\"\"\"\n",
        "conv_8 = tf.keras.layers.Conv2D(7, (1,1), strides= (1,1))(conv_7)\n",
        "\n",
        "\"\"\"\n",
        "Finally, global average pooling is applied to aggregate predictions at all spatial locations. \n",
        "\"\"\"\n",
        "avg_pool = tf.keras.layers.GlobalAveragePooling2D()(conv_8)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[input_A, depth_A,  input_B, depth_B], outputs=avg_pool)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT1HSaxaGanK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "2c31b880-84fb-49fd-9743-0295dcd5f98f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "first_image (InputLayer)        [(None, 480, 640, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "first_depth (InputLayer)        [(None, 480, 640, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "second_image (InputLayer)       [(None, 480, 640, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "second_depth (InputLayer)       [(None, 480, 640, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 480, 640, 8)  0           first_image[0][0]                \n",
            "                                                                 first_depth[0][0]                \n",
            "                                                                 second_image[0][0]               \n",
            "                                                                 second_depth[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 237, 317, 16) 6288        concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 117, 157, 32) 12832       conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 58, 78, 64)   18496       conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 28, 38, 128)  73856       conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 13, 18, 256)  295168      conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 6, 8, 512)    1180160     conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 2, 3, 1024)   4719616     conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 2, 3, 7)      7175        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 7)            0           conv2d_31[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,313,591\n",
            "Trainable params: 6,313,591\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKXx_R9GgXH",
        "colab_type": "text"
      },
      "source": [
        "**Declare the optimizer and loss function, then compile your *less ridiculous*  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTDWZaewGtPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer=_optimizer, loss = mean_psuedo_huber,  metrics=[metric_var, mean_sq_err])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zUadpkHe9j",
        "colab_type": "text"
      },
      "source": [
        "**Train using the fit_generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vv725GgzZCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6YZgC4hzaNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = \"./drive/My Drive/SpeedChallenge/Logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, update_freq=1250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXBgCxc40B-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"./drive/My Drive/SpeedChallenge/CheckFreiburg/removed_activations_changed_pool{epoch}.h5\", save_weights_only=True,  verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHc7Sk3IHiL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "96c22bca-1efb-4b4e-974a-6b0a9105a988"
      },
      "source": [
        "#New from Kyle:\n",
        "model.fit(training_generator, epochs=4, verbose=1, initial_epoch=0, validation_data = validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "59/59 [==============================] - 103s 2s/step - loss: 0.0585 - metric_var: 35.4881 - mean_sq_err: 0.1171 - val_loss: 0.0573 - val_metric_var: 30.5160 - val_mean_sq_err: 0.1148\n",
            "Epoch 2/4\n",
            "59/59 [==============================] - 104s 2s/step - loss: 0.0563 - metric_var: 27.0333 - mean_sq_err: 0.1128 - val_loss: 0.0552 - val_metric_var: 23.6313 - val_mean_sq_err: 0.1106\n",
            "Epoch 3/4\n",
            "59/59 [==============================] - 104s 2s/step - loss: 0.0544 - metric_var: 21.3138 - mean_sq_err: 0.1089 - val_loss: 0.0534 - val_metric_var: 19.0340 - val_mean_sq_err: 0.1070\n",
            "Epoch 4/4\n",
            "59/59 [==============================] - 104s 2s/step - loss: 0.0526 - metric_var: 17.2160 - mean_sq_err: 0.1054 - val_loss: 0.0518 - val_metric_var: 15.5486 - val_mean_sq_err: 0.1037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0f4853ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju5aY1Hy0fWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1 = np.load(\"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/images/rgbd_10760.npy\")/255.\n",
        "x2 = np.load(\"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/images/rgbd_10765.npy\")/255.\n",
        "p1 = np.load(\"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/pose/htm_10760.npy\")\n",
        "p2 = np.load(\"./drive/My Drive/Machine_Learning_Projects/freiburg_dataset/pose/htm_10765.npy\")\n",
        "delta_p = np.linalg.inv(p1).dot(p2)\n",
        "q = tfg_transformation.quaternion.from_rotation_matrix(delta_p[0:3, 0:3])\n",
        "print(q)\n",
        "print(delta_p[0:3,3])\n",
        "y = model.predict([x1[:,:,0:3].reshape(1,480,640,3), x1[:,:,3].reshape(1,480,640,1), x2[:,:,0:3].reshape(1,480,640,3), x2[:,:,3].reshape(1,480,640,1)])\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neQ1apRs0gCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}